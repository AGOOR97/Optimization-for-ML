# Optimization-for-ML

These NoteBooks are for ML Optimization using many techniques
such as: <br /> 

1- Gradient Descent <br /> 
2- Mini Batch Gradient Descent <br /> 
3- Stochastic Gradient Descent <br /> 
4- Momentum Based GD <br /> 
5- NAG <br /> 
6- Adagrad <br /> 
7- RMSProp <br /> 
8- Adam
